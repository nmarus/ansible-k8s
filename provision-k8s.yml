---
#
# Install HAProxy Loadbalancer for manager API access
#

- hosts: k8s_loadbalancer_manager
  tasks:
    - import_role:
        name: ubuntu-haproxy
      tags:
        - loadbalancer-manager
      vars:
        haproxy_frontend_mode: tcp
        haproxy_frontend_tcp_port: 6443
        haproxy_backend_mode: tcp
        haproxy_backend_port: 6443
        haproxy_backend_balance: leastconn
        haproxy_backend_servers: "{{groups['k8s_manager']|list}}"
        haproxy_app_domain_name: "{{network_domain_name}}"

#
# Install HAProxy Loadbalancer for router http/https access
#

- hosts: k8s_loadbalancer_router
  tasks:
    - import_role:
        name: ubuntu-haproxy
      tags:
        - loadbalancer-router
      vars:
        haproxy_frontend_mode: http
        haproxy_redirect_http: true
        haproxy_stats_enable: true
        haproxy_frontend_tcp_port: 80
        haproxy_frontend_tls_port: 443
        haproxy_backend_mode: https
        haproxy_backend_port: 443
        haproxy_backend_balance: roundrobin
        haproxy_backend_servers: "{{groups['k8s_router']|list}}"
        haproxy_app_domain_name: "{{network_domain_name}}"

#
# Setup GlusterFS Service (run on all glusterfs nodes)
#

- hosts: k8s_glusterfs
  tasks:
    - import_role:
        name: ubuntu-glusterfs
      tags:
        - glusterfs
      vars:
        glusterfs_nodes: "{{groups['k8s_glusterfs']|list}}"

#
# Setup Heketi on GlusterFS nodes (run on all glusterfs nodes)
#

- hosts: k8s_glusterfs
  tasks:
    - import_role:
        name: ubuntu-heketi
      tags:
        - heketi
      vars:
        glusterfs_nodes: "{{groups['k8s_glusterfs']|list}}"
        glusterfs_heketi_auth_allow: "{{(groups['k8s_manager'] + groups['k8s_worker']) | list}}"
        glusterfs_heketi_xfs_device: /dev/sdb
        glusterfs_heketi_gfs_volume: heketi_data

#
# Install k8s etcd cluster (run on all etcd nodes)
#

- hosts: k8s_etcd
  tasks:
    - import_role:
        name: ubuntu-k8s-etcd
      tags:
        - k8s-etcd
      vars:
        k8s_etcd_nodes: "{{groups['k8s_etcd']}}"


#
# Install k8s manager and workers (run on all k8s cluster nodes)
#

- hosts: k8s_manager:k8s_worker:k8s_router
  tasks:
    - import_role:
        name: ubuntu-k8s-install
      tags:
        - k8s-install
      vars:
        k8s_etcd_nodes: "{{groups['k8s_etcd']}}"
        k8s_manager_nodes: "{{groups['k8s_manager']}}"
        k8s_worker_nodes: "{{groups['k8s_worker'] + groups['k8s_router']}}"
        load_balancer_dns: "{{groups['k8s_loadbalancer_manager'][0]}}"
        vmware_node_group_name: vmware-ca-k8s

#
# Setup k8s to use Gluster for storage (run on all k8s cluster nodes)
#

- hosts: k8s_manager:k8s_worker:k8s_router
  tasks:
    - import_role:
        name: ubuntu-k8s-storage
      tags:
        - k8s-storage
      vars:
        k8s_controller: "{{groups['k8s_manager'][0]}}"
        glusterfs_namespace: glusterfs
        glusterfs_services_name: glusterfs
        glusterfs_nodes: "{{groups['k8s_glusterfs']|list}}"
        glusterfs_heketi_gfs_volume: heketi_data
        heketi_namespace: glusterfs
        heketi_services_name: heketi
        heketi_user_secret: "{{default_password}}"
        heketi_admin_secret: "{{default_password}}"

        #
        # storage class config
        #

        # type of gluster volume to use
        heketi_volumetype: "replicate:{{groups['k8s_glusterfs']|list|length}}"
        # Retain or Delete when dynamic volume no longer has claims
        heketi_reclaimPolicy: Delete
        # permit volume to be expanded after creating
        heketi_allowVolumeExpansion: true

        #
        # heketi topology config
        #

        # uninitialized block devices on glusterfs nodes
        heketi_xfs_devices:
          - /dev/sdc
        # initialize device when adding?
        heketi_destroy_device: true

#
# Label and Taint Router Nodes to restrict workloads
#

- hosts: k8s_manager
  tags:
    - k8s-tag-routers
  tasks:
    - include_role:
        name: ubuntu-k8s-taint-node
      run_once: true
      vars:
        node_ip_address: "{{item}}"
        node_taint: "routernode:NoSchedule"
      loop_control:
        extended: yes
      loop: "{{groups['k8s_router']}}"
    - include_role:
        name: ubuntu-k8s-label-node
      run_once: true
      vars:
        node_ip_address: "{{item}}"
        node_label: "role=router"
      loop_control:
        extended: yes
      loop: "{{groups['k8s_router']}}"

#
# Setup k8s ingress with haproxy (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-ingress-haproxy
      run_once: true
      tags:
        - never
        - k8s-ingress-haproxy
      vars:
        haproxy_node_label: role
        haproxy_node_label_value: router

#
# Setup k8s ingress with traefik (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-ingress-traefik
      run_once: true
      tags:
        - never
        - k8s-ingress-traefik

#
# Setup k8s ingress with nginx (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-ingress-nginx
      run_once: true
      tags:
        - never
        - k8s-ingress-nginx
      vars:
        nginx_node_label: role
        nginx_node_label_value: router

#
# Setup k8s descheduler (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-descheduler
      run_once: true
      tags:
        - never
        - k8s-descheduler
      vars:
        descheduler_rebalance_minutes: 5 # every 5 minutes
        # https://github.com/kubernetes-sigs/descheduler
        descheduler_strategy_RemovePodsViolatingInterPodAntiAffinity:
          enabled: true
        descheduler_strategy_RemovePodsViolatingNodeTaints:
          enabled: true
        descheduler_strategy_LowNodeUtilization:
          enabled: true
          params:
            nodeResourceUtilizationThresholds:
              thresholds:
                cpu : 20
                memory: 20
                pods: 20
              targetThresholds:
                cpu : 50
                memory: 50
                pods: 50
        descheduler_strategy_RemovePodsHavingTooManyRestarts:
          enabled: true
          params:
            podsHavingTooManyRestarts:
              podRestartThreshold: 100
              includingInitContainers: true

#
# Setup k8s quotas in default namespace (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-quotalimit
      run_once: true
      tags:
        - never
        - k8s-quotalimit
      vars:
        quota_namespace: default
        quota_services_name: quotalimits-default
        requests_cpu: 20000m
        requests_memory: 48Gi
        limits_cpu: 40000m
        limits_memory: 96Gi

#
# Setup k8s docker registry (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-registry
      run_once: true
      tags:
        - never
        - k8s-registry
      vars:
        registry_ssl_generate: true
        registry_ssl_domain: "registry.{{network_domain_name}}"
        registry_ssl_local_save: "./certs"

#
# Setup k8s prometheus (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-prometheus
      run_once: true
      tags:
        - never
        - k8s-prometheus
      vars:
        # namespaces for objects generated
        prometheus_namespace: monitoring
        blackbox_namespace: monitoring
        adapter_namespace: monitoring
        # names used to generate objects
        prometheus_service_name: prometheus-service
        blackbox_service_name: prometheus-blackbox
        adapter_service_name: prometheus-adapter
        # tls config (requires tls-secret to exist in default namespace)
        prometheus_tls_secret: "{{network_domain_name}}.tls-secret"
        blackbox_tls_secret: "{{network_domain_name}}.tls-secret"
        adapter_tls_secret: "{{network_domain_name}}.tls-secret"
        # external name for web interfaces
        prometheus_fqdn: prometheus.{{network_domain_name}}
        blackbox_fqdn: blackbox.{{network_domain_name}}

#
# Setup k8s grafana (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-grafana
      run_once: true
      tags:
        - never
        - k8s-grafana
      vars:
        grafana_namespace: monitoring
        grafana_services_name: grafana
        grafana_fqdn: grafana.{{network_domain_name}}
        grafana_tls_secret: "{{network_domain_name}}.tls-secret"

#
# Setup k8s dashboard (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-dashboard
      run_once: true
      tags:
        - never
        - k8s-dashboard
      vars:
        dashboard_namespace: kubernetes-dashboard
        dashboard_services_name: kubernetes-dashboard
        dashboard_fqdn: dashboard.{{network_domain_name}}
        dashboard_tls_secret: "{{network_domain_name}}.tls-secret"

#
# Setup k8s portainer (run on one k8s manager)
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-portainer
      run_once: true
      tags:
        - never
        - k8s-portainer
      vars:
        portainer_namespace: portainer
        portainer_services_name: portainer
        portainer_fqdn: portainer.{{network_domain_name}}
        portainer_tls_secret: "{{network_domain_name}}.tls-secret"

#
# Set k8s Cluster Autoscaling
#

- hosts: k8s_manager
  tasks:
    - import_role:
        name: ubuntu-k8s-vmw-cluster-autoscaler
      tags:
        - never
        - k8s-cas
      run_once: true
      vars:
        vmware_network_domain: "{{network_domain_name}}"
        vmware_network_name: "{{vcenter_network}}"
        vmware_network_dns:
          - "8.8.8.8"
          - "8.8.4.4"
        vmware_vcenter_host: "https://{{vcenter_hostname}}"
        vmware_vcenter_user: "{{vcenter_username}}"
        vmware_vcenter_pass: "{{vcenter_password}}"
        vmware_vcenter_insecure: "true"
        vmware_vcenter_datacenter: "{{vcenter_datacenter}}"
        vmware_vcenter_datastore: "{{vcenter_datastore}}"
        vmware_vcenter_resourcepool: k8s-worker-1-cas
        vmware_vm_name: "{{worker_prefix}}1"
        vmware_target_image: "{{vmw_base_image}}"
        vmware_max_nodes: 10
        # k8s cluster
        cluster_node_name: "{{worker_prefix}}1"
        cluster_node_group_name: "{{worker_prefix}}1-cas"
        cluster_manager_host: "{{groups['k8s_loadbalancer_manager'][0]}}"
        cluster_manager_port: 6443
        cluster_ssh_private_key_path: ".ssh/id_rsa"
        # custom docker images
        docker_image_vas: "{{registry_internalUri}}/nmarus/vsphere-autoscaler:v1.18.2"
        docker_image_cas: "{{registry_internalUri}}/nmarus/cluster-autoscaler:v1.18.2"

#
# Utility Playbooks
#

# Unset Install Facts
- hosts: all
  tasks:
    - name: Update Facts "k8s_enabled" to false
      set_fact:
        cacheable: yes
        k8s_enabled: false
      tags:
        - never
        - reset-install

# Setup kubectl on localhost for connectivity to cluster
- hosts: localhost
  tasks:
    - name: Remove existing kubectl config
      tags:
        - never
        - local-kube-config
      file:
        path: "{{lookup('env','HOME')}}/.kube"
        state: absent
    - name: Copy kubectl config from k8s manager node to localhost
      tags:
        - never
        - local-kube-config
      shell: |
        scp -q \
          -o StrictHostKeyChecking=no \
          -o UserKnownHostsFile=/dev/null \
          -i {{playbook_dir}}/{{ansible_ssh_private_key_file}} \
          -r {{ansible_user}}@{{groups['k8s_manager'][0]}}:/home/{{ansible_user}}/.kube \
          {{lookup('env','HOME')}}/
